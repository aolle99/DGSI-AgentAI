{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Agente Iterativo con LM Studio API (qwen3-8b)\n",
    "\n",
    "Este notebook implementa un agente simple que itera sobre un prompt inicial utilizando la API de LM Studio con el modelo qwen3-8b. El agente está diseñado para razonar y mejorar sus respuestas en cada iteración.\n"
   ],
   "id": "c0a56cdbdd6b95a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:57:52.452499Z",
     "start_time": "2025-05-27T14:57:52.247444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n"
   ],
   "id": "9f1c4311b9a43f0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuración de la API de LM Studio\n",
    "\n",
    "LM Studio proporciona una API compatible con OpenAI. Configuramos la URL base y los parámetros para conectarnos al modelo qwen3-8b.\n"
   ],
   "id": "cc9b98589c0129d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:57:55.661359Z",
     "start_time": "2025-05-27T14:57:55.657528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuración de la API\n",
    "API_URL = \"http://localhost:1234/v1/chat/completions\"  # URL por defecto de LM Studio\n",
    "\n",
    "# Parámetros para el modelo\n",
    "MODEL_NAME = \"qwen3-8b\"  # Nombre del modelo en LM Studio\n",
    "MAX_TOKENS = 1024        # Máximo de tokens en la respuesta\n",
    "TEMPERATURE = 0.7        # Temperatura para la generación (0.0-1.0)\n",
    "TOP_P = 0.9              # Top-p para la generación (0.0-1.0)\n",
    "MAX_ITERATIONS = 5       # Número máximo de iteraciones del agente\n"
   ],
   "id": "7e6a0fee1881507d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Función para llamar a la API de LM Studio\n",
    "\n",
    "Esta función envía una solicitud a la API de LM Studio y devuelve la respuesta generada por el modelo.\n"
   ],
   "id": "748df96facb3186d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:58:00.618666Z",
     "start_time": "2025-05-27T14:58:00.613140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def call_lm_studio_api(messages):\n",
    "    \"\"\"\n",
    "    Llama a la API de LM Studio con los mensajes proporcionados.\n",
    "\n",
    "    Args:\n",
    "        messages (list): Lista de mensajes en formato de chat para la API.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto de la respuesta generada por el modelo.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=data)\n",
    "        response.raise_for_status()  # Lanza una excepción si hay un error HTTP\n",
    "\n",
    "        result = response.json()\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al llamar a la API: {e}\")\n",
    "        return None\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Error al procesar la respuesta: {e}\")\n",
    "        print(f\"Respuesta recibida: {response.text}\")\n",
    "        return None\n"
   ],
   "id": "5c346735074ab32e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementación del Agente Iterativo\n",
    "\n",
    "El agente toma un prompt inicial y lo refina en cada iteración para mejorar la respuesta.\n"
   ],
   "id": "be8b88c03f266590"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:58:03.659791Z",
     "start_time": "2025-05-27T14:58:03.650360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def agente_iterativo(prompt_inicial, max_iteraciones=MAX_ITERATIONS):\n",
    "    \"\"\"\n",
    "    Implementa un agente que itera sobre un prompt inicial para mejorar la respuesta.\n",
    "\n",
    "    Args:\n",
    "        prompt_inicial (str): El prompt inicial para el agente.\n",
    "        max_iteraciones (int): Número máximo de iteraciones.\n",
    "\n",
    "    Returns:\n",
    "        dict: Historial de todas las iteraciones y la respuesta final.\n",
    "    \"\"\"\n",
    "    historial = []\n",
    "    prompt_actual = prompt_inicial\n",
    "\n",
    "    # Mensaje inicial del sistema para guiar al modelo\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Eres un asistente útil que razona paso a paso para resolver problemas. \" +\n",
    "                   \"Analiza cuidadosamente cada pregunta y proporciona respuestas detalladas y precisas. \" +\n",
    "                   \"Si no estás seguro de algo, indica qué información adicional necesitarías.\"\n",
    "    }\n",
    "\n",
    "    print(f\"Iteración 0 (Prompt Inicial): {prompt_inicial}\\n\")\n",
    "\n",
    "    for i in range(1, max_iteraciones + 1):\n",
    "        print(f\"\\n--- Iteración {i} ---\")\n",
    "\n",
    "        # Preparar mensajes para la API\n",
    "        messages = [system_message]\n",
    "\n",
    "        # Añadir historial de conversación si existe\n",
    "        for entry in historial:\n",
    "            messages.append({\"role\": \"user\", \"content\": entry[\"prompt\"]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": entry[\"respuesta\"]})\n",
    "\n",
    "        # Añadir el prompt actual\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt_actual})\n",
    "\n",
    "        # Llamar a la API\n",
    "        print(\"Generando respuesta...\")\n",
    "        start_time = time.time()\n",
    "        respuesta = call_lm_studio_api(messages)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if respuesta is None:\n",
    "            print(\"No se pudo obtener una respuesta. Terminando iteraciones.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Tiempo de respuesta: {end_time - start_time:.2f} segundos\")\n",
    "        print(f\"\\nRespuesta:\\n{respuesta}\\n\")\n",
    "\n",
    "        # Guardar esta iteración en el historial\n",
    "        historial.append({\n",
    "            \"iteracion\": i,\n",
    "            \"prompt\": prompt_actual,\n",
    "            \"respuesta\": respuesta\n",
    "        })\n",
    "\n",
    "        # Verificar si debemos continuar iterando\n",
    "        if i < max_iteraciones:\n",
    "            # Generar un nuevo prompt para refinar la respuesta\n",
    "            refinamiento_prompt = (\n",
    "                \"Basándote en tu respuesta anterior, ¿hay algún aspecto que puedas mejorar o profundizar? \" +\n",
    "                \"Si crees que tu respuesta es completa, indica 'RESPUESTA FINAL' al principio de tu respuesta. \" +\n",
    "                \"De lo contrario, proporciona una respuesta mejorada y más completa.\"\n",
    "            )\n",
    "\n",
    "            messages.append({\"role\": \"assistant\", \"content\": respuesta})\n",
    "            messages.append({\"role\": \"user\", \"content\": refinamiento_prompt})\n",
    "\n",
    "            print(\"Evaluando si la respuesta necesita refinamiento...\")\n",
    "            evaluacion = call_lm_studio_api(messages)\n",
    "\n",
    "            if evaluacion and evaluacion.startswith(\"RESPUESTA FINAL\"):\n",
    "                print(\"\\nEl agente considera que ha llegado a una respuesta final.\")\n",
    "                break\n",
    "\n",
    "            # Actualizar el prompt para la siguiente iteración\n",
    "            prompt_actual = (\n",
    "                f\"Basándote en el prompt original: '{prompt_inicial}', \" +\n",
    "                f\"y tu respuesta anterior, proporciona una respuesta mejorada y más completa.\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\nPreparando siguiente iteración con nuevo prompt: {prompt_actual}\")\n",
    "        else:\n",
    "            print(\"\\nSe ha alcanzado el número máximo de iteraciones.\")\n",
    "\n",
    "    # Devolver el historial completo y la respuesta final\n",
    "    resultado = {\n",
    "        \"prompt_inicial\": prompt_inicial,\n",
    "        \"historial\": historial,\n",
    "        \"respuesta_final\": historial[-1][\"respuesta\"] if historial else None,\n",
    "        \"iteraciones_totales\": len(historial)\n",
    "    }\n",
    "\n",
    "    return resultado\n"
   ],
   "id": "5edfca303a15b232",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Función para verificar la conexión con LM Studio\n",
    "\n",
    "Antes de usar el agente, es importante verificar que LM Studio esté ejecutándose y que el modelo qwen3-8b esté cargado.\n"
   ],
   "id": "4dbb7f261a5cfc52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:58:08.814800Z",
     "start_time": "2025-05-27T14:58:08.806559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def verificar_conexion_lm_studio():\n",
    "    \"\"\"\n",
    "    Verifica que LM Studio esté ejecutándose y que se pueda conectar a la API.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si la conexión es exitosa, False en caso contrario.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mensaje simple para probar la conexión\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hola, ¿estás funcionando?\"}\n",
    "        ]\n",
    "\n",
    "        respuesta = call_lm_studio_api(messages)\n",
    "\n",
    "        if respuesta:\n",
    "            print(f\"✅ Conexión exitosa con LM Studio. Respuesta de prueba: '{respuesta[:50]}...'\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ No se pudo obtener una respuesta de LM Studio.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al conectar con LM Studio: {e}\")\n",
    "        print(\"\\nAsegúrate de que:\")\n",
    "        print(\"1. LM Studio esté ejecutándose en tu computadora\")\n",
    "        print(\"2. El modelo qwen3-8b esté cargado en LM Studio\")\n",
    "        print(\"3. El servidor de la API esté activo en http://localhost:1234\")\n",
    "        print(\"4. No haya un firewall bloqueando la conexión\")\n",
    "        return False\n"
   ],
   "id": "2b6691b46e2a6772",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instrucciones para usar LM Studio\n",
    "\n",
    "Antes de ejecutar el agente, asegúrate de seguir estos pasos:\n",
    "\n",
    "1. Descarga e instala LM Studio desde [https://lmstudio.ai/](https://lmstudio.ai/)\n",
    "2. Abre LM Studio y descarga el modelo qwen3-8b\n",
    "3. Carga el modelo en LM Studio\n",
    "4. Inicia el servidor de la API en LM Studio (generalmente en http://localhost:1234)\n",
    "5. Ejecuta la celda de verificación de conexión a continuación\n"
   ],
   "id": "ffad937483df080f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:59:22.116321Z",
     "start_time": "2025-05-27T14:58:13.902919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verificar la conexión con LM Studio antes de usar el agente\n",
    "conexion_exitosa = verificar_conexion_lm_studio()\n",
    "\n",
    "if not conexion_exitosa:\n",
    "    print(\"\\nPor favor, configura LM Studio correctamente antes de continuar.\")\n"
   ],
   "id": "29eebcbd15472267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexión exitosa con LM Studio. Respuesta de prueba: '<think>\n",
      "Okay, the user asked \"Hola, ¿estás funcion...'\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ejemplo de uso del Agente Iterativo\n",
    "\n",
    "A continuación, puedes probar el agente con un prompt inicial. El agente iterará sobre el prompt para mejorar la respuesta.\n"
   ],
   "id": "52f173ce399ae752"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-27T14:59:56.558481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ejemplo de prompt inicial\n",
    "prompt_ejemplo = \"¿Cuáles son las principales causas del cambio climático y qué soluciones existen?\"\n",
    "\n",
    "# Puedes cambiar este prompt por cualquier otro que desees\n",
    "prompt_usuario = prompt_ejemplo\n",
    "\n",
    "# Ejecutar el agente iterativo (solo si la conexión fue exitosa)\n",
    "if conexion_exitosa:\n",
    "    resultado = agente_iterativo(prompt_usuario)\n",
    "\n",
    "    print(\"\\n=== Resumen del Proceso ===\")\n",
    "    print(f\"Prompt inicial: {resultado['prompt_inicial']}\")\n",
    "    print(f\"Número total de iteraciones: {resultado['iteraciones_totales']}\")\n",
    "    print(\"\\n=== Respuesta Final ===\\n\")\n",
    "    print(resultado['respuesta_final'])\n"
   ],
   "id": "464d03ee6cde18ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 0 (Prompt Inicial): ¿Cuáles son las principales causas del cambio climático y qué soluciones existen?\n",
      "\n",
      "\n",
      "--- Iteración 1 ---\n",
      "Generando respuesta...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Personalización del Agente\n",
    "\n",
    "Puedes personalizar el comportamiento del agente modificando los siguientes parámetros:\n"
   ],
   "id": "76b5ac0bfee409e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Personaliza estos parámetros según tus necesidades\n",
    "parametros_personalizados = {\n",
    "    \"api_url\": \"http://localhost:1234/v1/chat/completions\",  # URL de la API de LM Studio\n",
    "    \"model_name\": \"qwen3-8b\",                               # Nombre del modelo\n",
    "    \"max_tokens\": 1024,                                     # Máximo de tokens en la respuesta\n",
    "    \"temperature\": 0.7,                                     # Temperatura (0.0-1.0)\n",
    "    \"top_p\": 0.9,                                           # Top-p (0.0-1.0)\n",
    "    \"max_iterations\": 5                                     # Número máximo de iteraciones\n",
    "}\n",
    "\n",
    "# Para aplicar estos parámetros, ejecuta:\n",
    "# API_URL = parametros_personalizados[\"api_url\"]\n",
    "# MODEL_NAME = parametros_personalizados[\"model_name\"]\n",
    "# MAX_TOKENS = parametros_personalizados[\"max_tokens\"]\n",
    "# TEMPERATURE = parametros_personalizados[\"temperature\"]\n",
    "# TOP_P = parametros_personalizados[\"top_p\"]\n",
    "# MAX_ITERATIONS = parametros_personalizados[\"max_iterations\"]\n"
   ],
   "id": "38dc91f31c15ea66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Función para guardar los resultados\n",
    "\n",
    "Esta función te permite guardar los resultados del agente para su posterior análisis.\n"
   ],
   "id": "55fe81b96f0f3cbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def guardar_resultados(resultado, nombre_archivo=\"resultados_agente.json\"):\n",
    "    \"\"\"\n",
    "    Guarda los resultados del agente en un archivo JSON.\n",
    "\n",
    "    Args:\n",
    "        resultado (dict): Resultado del agente iterativo.\n",
    "        nombre_archivo (str): Nombre del archivo donde guardar los resultados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "            json.dump(resultado, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nResultados guardados exitosamente en '{nombre_archivo}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al guardar los resultados: {e}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "guardar_resultados(resultado)\n"
   ],
   "id": "30b558ed5548d9b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
